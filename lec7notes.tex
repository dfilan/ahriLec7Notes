\documentclass[twoside]{article}

%\usepackage[math]{kurier}
\usepackage[sc]{mathpazo}                   
%\renewcommand{\sfdefault}{kurier}

\usepackage{mathtools}

\usepackage{graphics}
\setlength{\oddsidemargin}{0.25 in}
\setlength{\evensidemargin}{-0.25 in}
\setlength{\topmargin}{-0.6 in}
\setlength{\textwidth}{6.5 in}
\setlength{\textheight}{8.5 in}
\setlength{\headsep}{0.75 in}
\setlength{\parindent}{0 in}
\setlength{\parskip}{0.1 in}


\newcounter{lecnum}
\renewcommand{\thepage}{\thelecnum-\arabic{page}}
\renewcommand{\thesection}{\thelecnum.\arabic{section}}
\renewcommand{\theequation}{\thelecnum.\arabic{equation}}
\renewcommand{\thefigure}{\thelecnum.\arabic{figure}}
\renewcommand{\thetable}{\thelecnum.\arabic{table}}


\newcommand{\lecture}[4]{
   \pagestyle{myheadings}
   \thispagestyle{plain}
   \newpage
   \setcounter{lecnum}{#1}
   \setcounter{page}{1}
   \noindent
   \begin{center}
   \framebox{
      \vbox{\vspace{2mm}
    \hbox to 6.28in { {\bf  CS 294-115 Algorithmic Human-Robot Interaction
                        \hfill Fall 2016} }
       \vspace{4mm}
       \hbox to 6.28in { {{\Large \hfill Lecture #1: #2  \hfill}} }
       \vspace{2mm}
       \hbox to 6.28in { {\it \hfill Scribes: #4} }
      \vspace{2mm}}
   }
   \end{center}
   \markboth{Lecture #1: #2}{Lecture #1: #2}

   \vspace*{4mm}
}
% what is the third argument to this command for?

\newcommand{\ts}{\textsuperscript}
\newcommand{\cu}{\mathcal{U}}
\newcommand{\cus}{\mathcal{U}_\text{smooth}}

%%%%%%%%%%%%%%%%%%%%%%%%%%
%document
\begin{document}
\lecture{7}{Trajectory Optimisation Part III}{}{Daniel Filan and Davis Foote}

\section{Summary}
This lecture will have two sections: first, an explanation of why and how to change the inner product implicitly used to define functional gradient descent, and secondly, a discussion of how student paper presentations will work, and why they will work that way.

\section{Changing the inner product}
\label{sec:chang-inner-prod}

For ease of understanding, we will think about our trajectories as discretised vectors, whose $i$\ts{th} component is the position in configuration space of the trajectory at the $i$\ts{th} point in (discrete) time. This vector will look something like
\begin{equation}
  \label{eq:1}
  \xi =
  \begin{bmatrix}
    q_1 \\
    q_2 \\
    q_3 \\
    \vdots \\
    q_N
  \end{bmatrix}
\end{equation}
Since we won't consider changing the start or end points of the trajectory, we leave out the initial configuration $q_0$ and the final configuration $q_{N+1}$. We should also remember that each element of this vector is itself a vector, rather than simply a scalar.

So far, we have been using the Euclidean inner product, defined as
\begin{equation}
  \label{eq:2}
  \langle \xi_1, \xi_2 \rangle_E = \xi_1^{\top} \xi_2 = q_{1,1}^{\top}q_{2,1} + \dotsb + q_{1,N}^\top q_{2,N}
\end{equation}

Why would we want to change this? Well, an inner product defines a norm by $\|\xi\| = \sqrt{\langle \xi, \xi \rangle}$, and a norm defines a distance metric defined by $\text{distance}(\xi_1, \xi_2) = \| \xi_1 - \xi_2 \|$. Therefore, we might ask ourselves whether the Euclidean metric induced by the Euclidean inner product is any good.

\subsection{Why the Euclidean metric isn't good}
\label{sec:why-euclidean-metric}

Consider three trajectories of a robot with one degree of freedom:

DIAGRAM HERE

\begin{equation}
  \label{eq:3}
  a =
  \begin{bmatrix}
    0 \\
    0 \\
    0 \\
    0 \\
    0
  \end{bmatrix},\, b =
  \begin{bmatrix}
    0 \\
    0 \\
    10 \\
    0 \\
    0
  \end{bmatrix}, \text{ and } c =
  \begin{bmatrix}
    0 \\
    5 \\
    10 \\
    5 \\
    0
  \end{bmatrix}
\end{equation}

Which is closer to $a$: $b$ or $c$? Well,
\begin{equation}
  \label{eq:4}
  \|a - b \|^2_E = \langle b, b \rangle_E = 10 \times 10 = 100
\end{equation}
and
\begin{equation}
  \label{eq:5}
  \|a - c \|^2_E = \langle c, c \rangle_E = 5 \times 5 + 10 \times 10 + 5 \times 5 = 150
\end{equation}
Therefore, according to the Euclidean metric, $b$ is closer to $a$ than $c$ is. However, intuitively, there's something wrong with this. $a$ involves very smooth motion, $b$ is super jerky, and $c$ involves a bit of a jerk, but less so than $b$. The Euclidean metric can't notice this because it doesn't take the orderings of elements of the vector into account, and therefore can't talk about speeds, but it would be nice if we could take time into account somehow.

Can we give a more rigorous argument for why we should change metric?

\subsubsection{Detour 1}
\label{sec:detour-1-1}

The formula for gradient descent, as we know, is $\xi_{i+1} = \xi_i - (1/\alpha)\nabla_{\xi_i}\cu$. But where does this come from?

Suppose instead that we approximate $\cu$ by its first order Taylor expansion, and try to minimise that. Then, we would use the rule
\begin{align}
  \label{eq:6}
  \xi_{i+1} &= \text{arg min}_{\xi} \left\{ \cu[\xi_i] + \nabla_{\xi_i}\cu^{\top} (\xi - \xi_i)\right\}\\
  \intertext{However, we know that this Taylor expansion will be off if we get far away in norm from $\xi_i$, so we add a regularisation term to penalise this:}
  \xi_{i+1} &= \text{arg min}_{\xi} \left\{ \cu[\xi_i] + \nabla_{\xi_i}\cu^{\top} (\xi - \xi_i) + \frac{1}{2} \alpha \|\xi - \xi_i \|_E^2 \right\}\label{eq:7} \\
  \intertext{This is a quadratic problem in $\xi$, so we can take the gradient of the term inside the curly brackets, set the gradient to zero, and thereby solve the problem. Doing this, we get}
  &\phantom{= \text{arg min}_\xi \Bigg\{\,\, }0 + \nabla_{\xi_i}\cu + \alpha (\xi - \xi_i)\label{eq:9}
\end{align}
Setting that expression to 0, we obtain
\begin{equation}
  \label{eq:10}
  \xi_{i+1} = \xi_i - \frac{1}{\alpha} \nabla_{\xi_i} \cu
\end{equation}
which is exactly the formula for gradient descent.

\subsubsection{Back to the main track}
\label{sec:back-main-track}

Remember the example of vectors $a$, $b$, and $c$? Well, gradient descent tends to go to things that are closer in norm, all else being equal. So, if $b$ is closer to $a$ than $c$ is, even if $c$ is better than $b$, gradient descent might well reach $b$ first, because of how much further away $c$ is.

What's the solution? We will change the distance metric (by changing the inner product) so that the sphere of elements of constant distance away from $a$ becomes an ellipsoid, thereby making $c$ closer than $b$.

DIAGRAM HERE

\subsection{How to make a new inner product}
\label{sec:how-make-new}

All inner products are of the form $\xi_1^\top A \xi_2$, where $A$ is a positive semi-definite matrix. Therefore, we just have to choose a matrix $A$. How?

Since we want our metric to like smooth deformations, let's think of other things that like that. Remember $\cus$? That was defined as
\begin{equation}
  \label{eq:11}
  \cus[\xi] = \frac{1}{2} \int_0^T \|\xi'(t)\|^2 dt
\end{equation}
When we discretise, the analogue of this is
\begin{equation}
  \cus[\xi] = \frac{1}{2} \sum_{i=0}^N \|q_{i+1} - q_i\|^2\label{eq:12}
\end{equation}

Suppose we compute the partial derivative of $\cus$ with respect to $q_i$. This is
\begin{equation}
  \label{eq:13}
  \frac{\partial}{\partial q_i}\cus = (\nabla_{\xi}\cus)_i = - (q_{i+1} - q_i) + (q_i - q_{i-1}) = 2q_i - q_{i-1} - q_{i+1}
\end{equation}
We're going to form a matrix $A$ such that
\begin{equation}
  \label{eq:14}
  \nabla_{\xi}\cus = A \xi + c
\end{equation}
where the constant $c$ will appear because $\nabla_\xi \cus$ depends on the endpoints $q_0$ and $q_{N+1}$, which don't appear in $\xi$. The matrix that does this is
\begin{equation}
  \label{eq:15}
  A =
  \begin{bmatrix}
    2 & -1 & & & & \\
    -1 & 2 & -1 & & & \\
    & -1 & 2 & -1 & & \\
    & & \ddots & \ddots & \ddots &  \\
    & & & -1 & 2 & -1 \\
    & & & & -1 & 2
  \end{bmatrix}
\end{equation}
This $A$ (the Hessian of $\cus$) will be what we use for our inner product:
\begin{equation}
  \label{eq:16}
  \langle \xi_1, \xi_2 \rangle_A = \xi_1^\top A \xi_2 \text{ and therefore } \|\xi_1 - \xi_2\|_A^2 = (\xi_1 - \xi_2)^\top A (\xi_1 - \xi_2)
\end{equation}

Going back to our motivational example, we now have that

\begin{align*}
  \|b - a\|^2_A &= \|b\|_A^2 \\
                &=
                  \begin{bmatrix}
                    0 & 0 & 10 & 0 & 0
                  \end{bmatrix}
                  \begin{bmatrix}
                    2 & -1 & 0 & 0 & 0 \\
                    -1 & 2 & -1 & 0 & 0 \\
                    0 & -1 & 2 & -1 & 0 \\
                    0 & 0 & -1 & 2 & -1 \\
                    0 & 0 & 0 & -1 & 2
                  \end{bmatrix}
                  \begin{bmatrix}
                    0 \\
                    0 \\
                    10 \\
                    0 \\
                    0
                  \end{bmatrix} \\
                &=\begin{bmatrix}
                    0 & 0 & 10 & 0 & 0
                  \end{bmatrix}
                  \begin{bmatrix}
                    0 \\
                    -10 \\
                    20 \\
                    -10 \\
                    0
                  \end{bmatrix} \\ \pagebreak[0]
                &= 200 \\
  \|c - a\|^2_A &=
                  \begin{bmatrix}
                    0 & 5 & 10 & 5 & 0
                  \end{bmatrix}
                  \begin{bmatrix}
                    2 & -1 & 0 & 0 & 0 \\
                    -1 & 2 & -1 & 0 & 0 \\
                    0 & -1 & 2 & -1 & 0 \\
                    0 & 0 & -1 & 2 & -1 \\
                    0 & 0 & 0 & -1 & 2
                  \end{bmatrix}
                  \begin{bmatrix}
                    0 \\
                    5 \\
                    10 \\
                    5 \\
                    0
                  \end{bmatrix} \\
                &=\begin{bmatrix}
                    0 & 5 & 10 & 5 & 0
                  \end{bmatrix}
                  \begin{bmatrix}
                    -5 \\
                    0 \\
                    10 \\
                    0 \\
                    -5
                  \end{bmatrix} \\
                &= 100 \\
\end{align*}

So it worked!

[there was a bit here where it talked about how gradient descent would now push a three-point trajectory to become uniform, but I don't see how that works on reflection, since we don't know what the cost function is, and that could well be higher for smoother gradients]

\subsection{How to do gradient descent now}
\label{sec:how-do-gradient}

blah blah blah

\end{document}
%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
